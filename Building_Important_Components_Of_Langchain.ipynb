{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvj4H2n2tg1mi1GyXhKlfe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SiddhantUke/OPENAI-AND-OLLAMA-/blob/main/Building_Important_Components_Of_Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Getting started With Langchain And Open AI\n",
        "\n",
        "In this quickstart we'll see how to:\n",
        "\n",
        "- Get setup with LangChain, LangSmith and LangServe\n",
        "- Use the most basic and common components of LangChain: prompt templates, models, and output parsers.\n",
        "- Build a simple application with LangChain\n",
        "- Trace your application with LangSmith\n",
        "- Serve your application with LangServe\n"
      ],
      "metadata": {
        "id": "qcj0QDkzJa8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "## Langsmith Tracking\n",
        "\n",
        "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
      ],
      "metadata": {
        "id": "0Yu-G5R_JYN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IN REQUIREMENT OF TXT file\n",
        "\n",
        "\n",
        "#### langchain\n",
        "#### python-dotenv\n",
        "#### lagchain-openai\n",
        "####\n",
        "\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "if we want to create a chat history then we want to create a chatopen ai api key and there is a another library as well for that !\n"
      ],
      "metadata": {
        "id": "f-VjnjJiKDz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "## specifically responsible for creating new object\n",
        "\n",
        "\n",
        "llm=ChatOpenAI(model=\"gpt-4o\")\n",
        "print(llm)"
      ],
      "metadata": {
        "id": "Ytb4kC0TJYKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "o/p ----> client=<openai.resources.chat.completions.Completions object at 0x000001FA49870E20> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001FA49870430> model_name='gpt-4o' openai_api_key=SecretStr('**********') openai_proxy=''"
      ],
      "metadata": {
        "id": "H8gDMtLKbUjK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### if we want to  ask any question to llm"
      ],
      "metadata": {
        "id": "YBpZXdjGMm3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Input and get response form LLM\n",
        "\n",
        "result=llm.invoke(\"What is generative AI?\")"
      ],
      "metadata": {
        "id": "OX0a9u-vJYII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "o/p -------> content='Generative AI refers to a subset of artificial intelligence that focuses on creating new content, such as text, images, audio, or even code, that resembles human-generated data. Unlike traditional AI systems that perform tasks based on predefined rules or patterns, generative AI models learn from vast amounts of data to generate new and original outputs.\\n\\nSome of the key techniques and models used in generative AI include:\\n\\n1. **Generative Adversarial Networks (GANs)**: These consist of two neural networks, a generator and a discriminator, that are trained together. The generator creates new data samples, while the discriminator evaluates them against real data. The goal is for the generator to produce data that the discriminator cannot distinguish from real data.\\n\\n2. **Variational Autoencoders (VAEs)**: These models encode input data into a compressed latent space and then decode it back to the original data space. By sampling from this latent space, new data samples can be generated.\\n\\n3. **Transformer Models**: Models like GPT (Generative Pre-trained Transformer) use transformer architecture to generate human-like text. They are trained on large datasets and can generate coherent and contextually relevant text based on a given prompt.\\n\\nGenerative AI has a wide range of applications, including:\\n\\n- **Text generation**: Creating articles, stories, and even code.\\n- **Image synthesis**: Generating realistic images, art, and designs.\\n- **Audio generation**: Producing music, voice, and sound effects.\\n- **Data augmentation**: Creating synthetic data to improve machine learning models.\\n\\nThe technology has enormous potential but also raises ethical and societal concerns, such as the creation of deepfakes, intellectual property issues, and the potential for misuse in generating misleading information.' response_metadata={'token_usage': {'completion_tokens': 351, 'prompt_tokens': 13, 'total_tokens': 364}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_f4e629d0a5', 'finish_reason': 'stop', 'logprobs': None} id='run-797b6d80-6f17-4c55-90cd-26ee1824301c-0' usage_metadata={'input_tokens': 13, 'output_tokens': 351, 'total_tokens': 364}"
      ],
      "metadata": {
        "id": "B5mjR0e5bekG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)"
      ],
      "metadata": {
        "id": "OntIJLyYJYE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for moniterong the response you can also check the langchain dashboard"
      ],
      "metadata": {
        "id": "secNJyTTXOte"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "\n",
        "ChatPromptTemplate is used to create structured prompts for language models (like ChatGPT) in LangChain, making it easy to insert dynamic variables into the prompt.\n"
      ],
      "metadata": {
        "id": "dJdSYGLOYYUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Chatprompt Template\n",
        "from langchain_core.prompts import ChatPromptTemplate  ## you have to behave like this !\n",
        "\n",
        "\n",
        "prompt=ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\",\"You are an expert AI Engineer. Provide me answers based on the questions\"), ## system = llm model\n",
        "        (\"user\",\"{input}\")\n",
        "    ]\n",
        "\n",
        ")\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOuGzxt2JYCz",
        "outputId": "47d9e287-9e32-4964-aca1-f95c3980595f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answers based on the questions'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## if we want to combine my prompt with particular model along with my prompt template then we have to something do like this !\n",
        "\n",
        "## chain\n",
        "chain=prompt|llm  ## along with llm and prompt combine\n",
        "\n",
        "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})  ## in order to call this ## chain\n",
        "print(response)"
      ],
      "metadata": {
        "id": "EHBV-VS9JYAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(response)\n"
      ],
      "metadata": {
        "id": "d_yEZyXEJX9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "o/p --> langchain_core.messages.ai.AIMessage"
      ],
      "metadata": {
        "id": "YGp_W_-CbKa0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yY3OOB_LcmHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OUTPUT PARSOR\n",
        "\n",
        "In LangChain, an Output Parser is used to convert the raw output of a language model into a structured format, like a dictionary, list, or custom object—something easier to work with in any application."
      ],
      "metadata": {
        "id": "J4f_ouMqbx-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## stroutput Parser\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "output_parser=StrOutputParser()\n",
        "chain=prompt|llm|output_parser\n",
        "\n",
        "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
        "print(response)"
      ],
      "metadata": {
        "id": "YmRelzxvJX7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O/P ---------> Certainly! As of my knowledge cutoff date in October 2023, \"Langsmith\" does not directly correlate to any widely recognized technology, framework, or concept in the field of AI or software engineering. It's possible that it might be a term used within a specific company, a proprietary tool, or a recent development that hasn't gained widespread recognition.\n",
        "\n",
        "However, if \"Langsmith\" is a term related to a specific context or a new product or service that emerged after my last update, I wouldn't have information on it. If you could provide more context or details, I'd be happy to help you understand the concept or direct you to the right resources!"
      ],
      "metadata": {
        "id": "9k4BuZQqcrmO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cJDqSKAWJX5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WTuMt1X6JX3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LqlDXK9BJX0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V_9kDJnHJXyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VObo0M7jJXvx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}